# üêç Reinforcement Learning f√ºr Snake ‚Äì Projektbericht

Dieses Projekt entstand im Rahmen der Lehrveranstaltung *"Aktuelle Data Science Entwicklungen"* an der Dualen Hochschule Baden-W√ºrttemberg (DHBW) im 6. Semester. Ziel war es, einen Reinforcement-Learning-Agenten zu entwickeln, der das klassische Spiel Snake eigenst√§ndig und effizient meistern kann. Als Kernalgorithmus kam Deep Q-Learning (DQN) zum Einsatz. Die Snake-Umgebung wurde daf√ºr eigenst√§ndig in Python unter Verwendung von `pygame` erstellt.

Die Implementierung des Agenten basiert auf PyTorch. Das neuronale Netz erh√§lt als Input den flachen Zustand der Umgebung und gibt Q-Werte f√ºr alle m√∂glichen Aktionen aus. Der Agent entscheidet sich entweder zuf√§llig (mit Wahrscheinlichkeit Œµ) oder w√§hlt die beste bekannte Aktion aus dem Netzwerk (mit 1‚ÄìŒµ). Der Epsilon-Wert wird dabei schrittweise reduziert, um den Lernprozess vom Explorieren hin zum Exploiteren zu steuern.

Um das Training zu beschleunigen, wurde auf eine visuelle Ausgabe w√§hrend des Trainings verzichtet. Das grafische Rendern mit `pygame` kann optional zugeschaltet werden, z.‚ÄØB. zur Evaluation eines trainierten Modells. Nach Abschluss des Trainings wird das Modell in einer `.pth`-Datei gespeichert und kann in einem separaten Skript (`play_trained.py`) geladen werden, um den Agenten automatisch spielen zu lassen. Dadurch l√§sst sich das Modell wiederverwenden, ohne das Training erneut durchlaufen zu m√ºssen.

Die Installation aller notwendigen Bibliotheken erfolgt √ºber die Datei `requirements.txt`. Nach erfolgreicher Installation kann das Training mit dem Skript `train_agent.py` gestartet werden. Das trainierte Modell wird anschlie√üend automatisch gespeichert. Zur Visualisierung der Lernkurve dient eine einfache Matplotlib-Grafik, die den Reward-Verlauf √ºber die Episoden hinweg darstellt.

### Projektstruktur

Das Projekt ist in folgende Komponenten unterteilt:

- `src/snake_env.py` enth√§lt die Snake-Umgebung
- `src/agent.py` enth√§lt die DQN-Implementierung
- `train_agent.py` f√ºhrt den Trainingsprozess aus
- `play_trained.py` l√§dt ein trainiertes Modell und spielt es ab
- `dqn_snake.pth` ist die gespeicherte Modell-Datei
- `README.md` enth√§lt die Projektdokumentation

### Verwendete Technologien

F√ºr das Projekt kamen folgende Bibliotheken und Technologien zum Einsatz:

- Python 3.10
- PyTorch
- pygame
- numpy
- matplotlib

### Aufgabenverteilung innerhalb der Gruppe

Das Projekt wurde in einer Zweiergruppe von **David** und **Paul** bearbeitet. Die Aufgabenverteilung orientierte sich an individuellen St√§rken und Interessen, wurde aber regelm√§√üig gemeinsam abgestimmt und abgestimmt umgesetzt.

Paul √ºbernahm schwerpunktm√§√üig die Entwicklung der Snake-Umgebung mit `pygame`, sowie die Strukturierung des Codes (Projektaufbau, Dateistruktur, Speicherpfade). Zus√§tzlich k√ºmmerte er sich um die Modell-Speicherung, das Render-Handling und die Integration der Trainingsvisualisierung.

David implementierte den DQN-Agenten inklusive Modellarchitektur, Replay Buffer, Epsilon-gesteuerter Aktionswahl und Target-Netzwerk. Au√üerdem betreute er das Training, die Hyperparameterwahl und die grafische Auswertung der Trainingsperformance. Die Visualisierung des Lernverlaufs und die Ergebnisinterpretation wurden ebenfalls durch ihn umgesetzt.

Beide Gruppenmitglieder haben sich gemeinsam mit der RL-Theorie, der Erstellung des wissenschaftlichen Berichts und dem Schreiben dieser Dokumentation befasst.


### Abschlie√üende Hinweis

Dieses Projekt dient ausschlie√ülich akademischen Zwecken im Rahmen der DHBW-Lehre. Alle verwendeten Bibliotheken sind Open Source.
